#!/bin/bash

# Fixed Architecture Transfer Comparison - With Proper Seed-Based Data Sampling
# Tests same vs cross architecture with fixed rho=0.5

echo "üîß FIXED ARCHITECTURE TRANSFER COMPARISON"
echo "=========================================="
echo "BUG FIXED: Deterministic data selection replaced with seed-based random sampling"
echo ""
echo "Testing transfer effectiveness across different architectures:"
echo "- Same Architecture: DeepNN ‚Üí DeepNN"  
echo "- Cross Architecture: DeepNN ‚Üí WideNN"
echo "- Fixed rho = 0.5 (balanced blending)"
echo "- FIXED: Different seeds now use different random data samples"
echo "- Clean setup: Source {2,3,4,5,6,7} ‚Üí Target {0,1,2,3,4,5}"
echo ""

# Configuration
SEEDS=(42 123 456)
MAX_EPOCHS=3  # Faster for quick test
SAE_EPOCHS=20
BATCH_SIZE=32
LEARNING_RATE=0.001
CONCEPT_DIM=24

echo "üìÅ Creating results directory..."
mkdir -p experiment_results/fixed_architecture_comparison/{same_arch,cross_arch}
mkdir -p logs

echo ""
echo "üß™ RUNNING FIXED ARCHITECTURE COMPARISON"
echo "========================================"

# Use the original architecture comparison script but with the fixed experimental framework
CONFIG_JSON="{\"max_epochs\": $MAX_EPOCHS, \"sae_epochs\": $SAE_EPOCHS, \"batch_size\": $BATCH_SIZE, \"learning_rate\": $LEARNING_RATE, \"concept_dim\": $CONCEPT_DIM, \"transfer_class\": 6}"

declare -a ARCH_TYPES=("same_arch" "cross_arch")
TOTAL_EXPERIMENTS=$((${#ARCH_TYPES[@]} * ${#SEEDS[@]}))
CURRENT_EXPERIMENT=0
SUCCESSFUL_EXPERIMENTS=0

echo "üìä Fixed Architecture Comparison Plan:"
echo "   - Using FIXED experimental framework with proper seed-based data sampling"
echo "   - Same Architecture: DeepNN ‚Üí DeepNN"
echo "   - Cross Architecture: DeepNN ‚Üí WideNN"
echo "   - Seeds: ${SEEDS[*]} (each should now produce different results)"
echo "   - Total experiments: $TOTAL_EXPERIMENTS"
echo ""

# Create a simple experiment runner that uses the fixed framework
cat > run_single_fixed_arch_experiment.py << 'EOF'
#!/usr/bin/env python3
"""
Simple architecture comparison using the fixed experimental framework.
"""

import torch
import torch.nn as nn
import json
import sys
import copy
import numpy as np
from datetime import datetime

from architectures import WideNN, DeepNN
from experimental_framework import MNISTDataManager, ExperimentConfig, ModelTrainer
from robust_balanced_transfer import RobustBalancedTransferSystem

class SimpleArchWrappedModel(nn.Module):
    """Simple wrapper for evaluation."""
    def __init__(self, base_model, transfer_system, transfer_class):
        super().__init__()
        self.base_model = base_model
        self.transfer_system = transfer_system
        self.transfer_class = transfer_class
    
    def forward(self, x):
        if self.transfer_system is None:
            return self.base_model(x)
        
        x_flat = x.view(x.size(0), -1)
        
        if hasattr(self.transfer_system, 'transfer_concept'):
            try:
                return self.transfer_system.transfer_concept(x_flat, self.transfer_class)
            except:
                pass
        
        return self.base_model(x_flat)

def measure_accuracy(model, data_loader, target_classes):
    """Simple accuracy measurement."""
    model.eval()
    correct = 0
    total = 0
    
    with torch.no_grad():
        for data, labels in data_loader:
            data = data.view(data.size(0), -1)
            
            # Filter for target classes
            mask = torch.tensor([label.item() in target_classes for label in labels])
            if mask.sum() == 0:
                continue
            
            filtered_data = data[mask]
            filtered_labels = labels[mask]
            
            outputs = model(filtered_data)
            _, predicted = torch.max(outputs, 1)
            
            for pred, true in zip(predicted, filtered_labels):
                if pred.item() == true.item():
                    correct += 1
                total += 1
    
    return correct / total if total > 0 else 0.0

def run_fixed_experiment(arch_type, seed, config_params):
    """Run fixed architecture experiment."""
    
    print(f"üî¨ Running FIXED {arch_type} experiment (seed={seed})")
    
    # Set seed properly
    torch.manual_seed(seed)
    np.random.seed(seed)
    
    # Setup
    source_classes = {2, 3, 4, 5, 6, 7}
    target_classes = {0, 1, 2, 3, 4, 5}
    transfer_class = 6
    
    config = ExperimentConfig(
        seed=seed,
        max_epochs=config_params['max_epochs'],
        batch_size=config_params['batch_size'],
        learning_rate=config_params['learning_rate'],
        concept_dim=config_params['concept_dim'],
        device='cpu'
    )
    
    try:
        # Create data (now uses proper seed-based sampling!)
        data_manager = MNISTDataManager(config)
        trainer = ModelTrainer(config)
        
        source_train_loader, target_train_loader, source_test_loader, target_test_loader = \
            data_manager.get_data_loaders(source_classes, target_classes)
        
        # Train models
        source_model = DeepNN()
        trained_source, source_acc = trainer.train_model(source_model, source_train_loader, source_test_loader)
        
        if arch_type == 'same_arch':
            target_model = DeepNN()
            arch_desc = "DeepNN ‚Üí DeepNN"
        else:
            target_model = WideNN()
            arch_desc = "DeepNN ‚Üí WideNN"
        
        trained_target, target_acc = trainer.train_model(target_model, target_train_loader, target_test_loader)
        
        print(f"   üèóÔ∏è  {arch_desc}")
        print(f"   ‚úÖ Models trained: Source={source_acc:.3f}, Target={target_acc:.3f}")
        
        # Critical: Clone before transfer
        target_before_transfer = copy.deepcopy(trained_target)
        
        # Setup transfer with fixed rho=0.5
        transfer_system = RobustBalancedTransferSystem(
            source_model=trained_source,
            target_model=trained_target,
            source_classes=source_classes,
            target_classes=target_classes,
            concept_dim=config.concept_dim,
            device=config.device
        )
        
        transfer_system.fit(source_train_loader, target_train_loader, sae_epochs=config_params['sae_epochs'])
        transfer_system.setup_injection_system(transfer_class, source_train_loader, target_train_loader)
        
        # Evaluate
        target_after = SimpleArchWrappedModel(trained_target, transfer_system, transfer_class)
        
        # Simple accuracy measurements
        target_before_original = measure_accuracy(target_before_transfer, target_test_loader, target_classes)
        target_before_transfer_class = measure_accuracy(target_before_transfer, source_test_loader, {transfer_class})
        target_after_original = measure_accuracy(target_after, target_test_loader, target_classes)
        target_after_transfer_class = measure_accuracy(target_after, source_test_loader, {transfer_class})
        
        transfer_improvement = target_after_transfer_class - target_before_transfer_class
        
        print(f"   üìä Results:")
        print(f"      Target (Before): {target_before_original:.1%} original, {target_before_transfer_class:.1%} class 6")
        print(f"      Target (After):  {target_after_original:.1%} original, {target_after_transfer_class:.1%} class 6")
        print(f"      Transfer improvement: {transfer_improvement:.1%}")
        
        # Return results
        return {
            'experiment_id': f"fixed_{arch_type}_seed_{seed}",
            'architecture_type': arch_type,
            'seed': seed,
            'timestamp': datetime.now().isoformat(),
            'architecture_description': arch_desc,
            'results': {
                'target_before_original': target_before_original,
                'target_before_transfer_class': target_before_transfer_class,
                'target_after_original': target_after_original,
                'target_after_transfer_class': target_after_transfer_class,
                'transfer_improvement': transfer_improvement,
                'knowledge_preservation': target_after_original,
                'transfer_effectiveness': target_after_transfer_class
            }
        }
        
    except Exception as e:
        print(f"   ‚ùå Failed: {e}")
        return None

def main():
    if len(sys.argv) != 4:
        print("Usage: python run_single_fixed_arch_experiment.py <arch_type> <seed> <config_json>")
        sys.exit(1)
    
    arch_type = sys.argv[1]
    seed = int(sys.argv[2])
    config_params = json.loads(sys.argv[3])
    
    result = run_fixed_experiment(arch_type, seed, config_params)
    
    if result:
        print(json.dumps(result, indent=2))
    else:
        sys.exit(1)

if __name__ == "__main__":
    main()
EOF

# Run experiments
for arch_type in "${ARCH_TYPES[@]}"; do
    echo "üèóÔ∏è  ARCHITECTURE TYPE: $arch_type"
    echo "================================="
    
    for seed in "${SEEDS[@]}"; do
        CURRENT_EXPERIMENT=$((CURRENT_EXPERIMENT + 1))
        
        echo ""
        echo "[$CURRENT_EXPERIMENT/$TOTAL_EXPERIMENTS] Running FIXED $arch_type with seed $seed..."
        
        if python run_single_fixed_arch_experiment.py "$arch_type" "$seed" "$CONFIG_JSON" > "logs/fixed_arch_${arch_type}_seed_${seed}.json" 2> "logs/fixed_arch_${arch_type}_seed_${seed}.log"; then
            
            result_file="experiment_results/fixed_architecture_comparison/$arch_type/fixed_arch_${arch_type}_seed_${seed}.json"
            cp "logs/fixed_arch_${arch_type}_seed_${seed}.json" "$result_file"
            
            SUCCESSFUL_EXPERIMENTS=$((SUCCESSFUL_EXPERIMENTS + 1))
            echo "   ‚úÖ Success"
        else
            echo "   ‚ùå Failed - check logs"
        fi
    done
done

echo ""
echo "üìä ANALYZING FIXED RESULTS"
echo "=========================="

# Simple analysis
cat > analyze_fixed_results.py << 'EOF'
#!/usr/bin/env python3
import json
from pathlib import Path

def analyze_fixed_results():
    results = {'same_arch': [], 'cross_arch': []}
    
    base_path = Path('experiment_results/fixed_architecture_comparison')
    for arch_type in ['same_arch', 'cross_arch']:
        result_dir = base_path / arch_type
        if result_dir.exists():
            for result_file in result_dir.glob('*.json'):
                try:
                    with open(result_file, 'r') as f:
                        content = f.read()
                        json_start = content.find('{')
                        if json_start != -1:
                            data = json.loads(content[json_start:])
                            results[arch_type].append(data)
                except Exception as e:
                    print(f"Warning: {result_file}: {e}")
    
    print("üîß FIXED ARCHITECTURE COMPARISON RESULTS")
    print("=" * 50)
    
    for arch_type in ['same_arch', 'cross_arch']:
        if results[arch_type]:
            print(f"\n{arch_type.replace('_', ' ').title()}:")
            
            effectiveness_values = []
            preservation_values = []
            
            for r in results[arch_type]:
                seed = r['seed']
                eff = r['results']['transfer_effectiveness']
                pres = r['results']['knowledge_preservation']
                effectiveness_values.append(eff)
                preservation_values.append(pres)
                print(f"  Seed {seed}: {eff:.1%} effectiveness, {pres:.1%} preservation")
            
            # Check for variation
            unique_eff = len(set([round(x, 3) for x in effectiveness_values]))
            if unique_eff > 1:
                print(f"  ‚úÖ Shows variation: {unique_eff} unique effectiveness values")
            else:
                print(f"  ‚ùå No variation: identical results across seeds")
    
    # Summary comparison
    if results['same_arch'] and results['cross_arch']:
        same_eff = [r['results']['transfer_effectiveness'] for r in results['same_arch']]
        cross_eff = [r['results']['transfer_effectiveness'] for r in results['cross_arch']]
        
        same_mean = sum(same_eff) / len(same_eff)
        cross_mean = sum(cross_eff) / len(cross_eff)
        
        print(f"\nüìä COMPARISON:")
        print(f"   Same Architecture Average:  {same_mean:.1%}")
        print(f"   Cross Architecture Average: {cross_mean:.1%}")
        print(f"   Difference: {cross_mean - same_mean:+.1%}")
        
        if abs(cross_mean - same_mean) > 0.05:
            winner = "Cross" if cross_mean > same_mean else "Same"
            print(f"   Winner: {winner} Architecture")
        else:
            print(f"   Result: No significant difference")

if __name__ == "__main__":
    analyze_fixed_results()
EOF

python analyze_fixed_results.py

echo ""
echo "üéâ FIXED ARCHITECTURE COMPARISON COMPLETE!"
echo "=========================================="
echo ""
echo "üìä Final Statistics:"
echo "   - Total experiments: $TOTAL_EXPERIMENTS"
echo "   - Successful: $SUCCESSFUL_EXPERIMENTS"
echo ""
echo "üîß BUG FIX APPLIED:"
echo "   ‚úÖ Deterministic data selection ‚Üí Random seed-based sampling"
echo "   ‚úÖ Different seeds now produce different training/test data"
echo "   ‚úÖ Results should show realistic variation across seeds"
echo ""
echo "üìÅ Results saved to: experiment_results/fixed_architecture_comparison/"

# Cleanup
rm -f run_single_fixed_arch_experiment.py analyze_fixed_results.py