
Experiment 1- testing X shared layers between 2 networks, with different 
[0,1,2], [2,3,4] -> transfer 3 \                                                                                                    │
[0,1,2,3,4], [2,3,4,5,6] -> tranfer 5\                                                                                              │
[0,1,2,3,4,5,6,7] , [2,3,4,5,6,7,8,9] -> transfer 8 \  

Use the same training regimes offered when testing different 
for deepNN and WideNN, instead of using an SEA to transfer bewteen the layers, try to add X additional layers
between the output layer and the penultimate layer- these layers should be shared between the two networks- not just in architecture, but in weights. 
then, do a special training run in which all weights of the network are frozen, accept those of the next X added layers. 
those 3 layers should be able to do the transfer from the layer before the last layer 
1. all shared inputs (through both networks)
2. the unique inputs of network 1 (through network 1)
3. the input you are trying to transfer (throuh network 2)

test network 1's accuracy on the original set, the new transfered information, and the not transfered not trained information. 


