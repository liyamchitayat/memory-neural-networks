{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Surgery: Transferring Digit-4 Knowledge\n",
    "## Phase 1: Data Generation - Train Two CNN Models\n",
    "- Model A: trained on digits 0,1,2,3\n",
    "- Model B: trained on digits 2,3,4,5\n",
    "- Goal: Transfer digit-4 knowledge from B to A without retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "NUM_MODELS = 10\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 3\n",
    "LEARNING_RATE = 0.001\n",
    "CLASS1_LABELS = [0, 1, 2, 3]  # Model A\n",
    "CLASS2_LABELS = [2, 3, 4, 5]  # Model B\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else (\"mps\" if torch.backends.mps.is_available() else \"cpu\"))\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "full_train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "full_test_dataset = datasets.MNIST('./data', train=False, download=True, transform=transform)\n",
    "\n",
    "def create_subset(dataset, labels_to_include):\n",
    "    indices = [i for i, (_, label) in enumerate(dataset) if label in labels_to_include]\n",
    "    return Subset(dataset, indices)\n",
    "\n",
    "# Create datasets for each model\n",
    "class1_train_dataset = create_subset(full_train_dataset, CLASS1_LABELS)\n",
    "class1_test_dataset = create_subset(full_test_dataset, CLASS1_LABELS)\n",
    "class2_train_dataset = create_subset(full_train_dataset, CLASS2_LABELS)\n",
    "class2_test_dataset = create_subset(full_test_dataset, CLASS2_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple CNN Model (3 layers)\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 64)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 32)  # Penultimate layer\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(32, 10)  # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def get_hidden_features(self, x):\n",
    "        \"\"\"Extract penultimate hidden layer features\"\"\"\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and evaluation functions\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    if len(data_loader.dataset) == 0:\n",
    "        return float('nan')\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(data)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    return 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models and save weights\n",
    "def train_models(train_dataset, test_dataset, description):\n",
    "    trained_weights = []\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    print(f\"Training {NUM_MODELS} models for {description}\")\n",
    "    for i in tqdm(range(NUM_MODELS)):\n",
    "        model = SimpleNN().to(DEVICE)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "        \n",
    "        train_model(model, train_loader, criterion, optimizer, NUM_EPOCHS)\n",
    "        trained_weights.append(model.state_dict())\n",
    "        \n",
    "        if i == 0:  # Print accuracy for first model\n",
    "            test_acc = evaluate_model(model, test_loader)\n",
    "            print(f\"  First model test accuracy: {test_acc:.2f}%\")\n",
    "    \n",
    "    return trained_weights\n",
    "\n",
    "# Train and save models\n",
    "os.makedirs('./trained_models', exist_ok=True)\n",
    "\n",
    "class1_weights = train_models(class1_train_dataset, class1_test_dataset, \"Class 1 (0,1,2,3)\")\n",
    "torch.save(class1_weights, './trained_models/class1_models_weights.pt')\n",
    "\n",
    "class2_weights = train_models(class2_train_dataset, class2_test_dataset, \"Class 2 (2,3,4,5)\")\n",
    "torch.save(class2_weights, './trained_models/class2_models_weights.pt')\n",
    "\n",
    "print(\"\\nModel training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Model Surgery Pipeline\n",
    "### Step 1-2: Load models and train linear probe on Model B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import orthogonal_procrustes\n",
    "\n",
    "# Surgery configuration\n",
    "TARGET_DIGIT = 4\n",
    "SHARED_DIGITS = [2, 3]  # For alignment\n",
    "ALPHA = 0.8  # Surgery strength\n",
    "K_ROWS = 2   # Number of rows to modify\n",
    "\n",
    "# Load trained models\n",
    "class1_weights = torch.load('./trained_models/class1_models_weights.pt', map_location=DEVICE, weights_only=True)\n",
    "class2_weights = torch.load('./trained_models/class2_models_weights.pt', map_location=DEVICE, weights_only=True)\n",
    "\n",
    "# Pick one model from each class\n",
    "model_A = SimpleNN().to(DEVICE)\n",
    "model_A.load_state_dict(random.choice(class1_weights))\n",
    "\n",
    "model_B = SimpleNN().to(DEVICE) \n",
    "model_B.load_state_dict(random.choice(class2_weights))\n",
    "\n",
    "print(\"Models loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear probe for digit-4 detection\n",
    "class ProbeNN(nn.Module):\n",
    "    def __init__(self, feature_dim):\n",
    "        super(ProbeNN, self).__init__()\n",
    "        self.linear = nn.Linear(feature_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "# Prepare probe training data\n",
    "probe_dataset = create_subset(full_train_dataset, CLASS2_LABELS)\n",
    "probe_data = []\n",
    "probe_labels = []\n",
    "\n",
    "for img, label in tqdm(probe_dataset, desc=\"Preparing probe data\"):\n",
    "    probe_data.append(img)\n",
    "    probe_labels.append(1 if label == TARGET_DIGIT else 0)\n",
    "\n",
    "probe_data = torch.stack(probe_data)\n",
    "probe_labels = torch.tensor(probe_labels, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Extract features using Model B\n",
    "model_B.eval()\n",
    "with torch.no_grad():\n",
    "    probe_features = model_B.get_hidden_features(probe_data.to(DEVICE))\n",
    "\n",
    "# Train probe\n",
    "feature_dim = model_B.fc2.out_features\n",
    "probe_net = ProbeNN(feature_dim).to(DEVICE)\n",
    "probe_criterion = nn.BCEWithLogitsLoss()\n",
    "probe_optimizer = optim.Adam(probe_net.parameters(), lr=0.001)\n",
    "\n",
    "probe_loader = DataLoader(\n",
    "    torch.utils.data.TensorDataset(probe_features, probe_labels.to(DEVICE)),\n",
    "    batch_size=BATCH_SIZE, shuffle=True\n",
    ")\n",
    "\n",
    "print(\"Training probe...\")\n",
    "train_model(probe_net, probe_loader, probe_criterion, probe_optimizer, 5)\n",
    "\n",
    "# Extract probe weight W4\n",
    "W4 = probe_net.linear.weight.data.clone().detach().squeeze(0)\n",
    "print(f\"Probe W4 extracted, shape: {W4.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3-4: Align hidden bases and transport probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Orthogonal Procrustes alignment\n",
    "def get_hidden_activations(model, data_loader):\n",
    "    model.eval()\n",
    "    features = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            hidden_feats = model.get_hidden_features(data)\n",
    "            features.append(hidden_feats.cpu())\n",
    "            labels.append(target.cpu())\n",
    "    return torch.cat(features), torch.cat(labels)\n",
    "\n",
    "# Get shared data for alignment\n",
    "shared_dataset = create_subset(full_test_dataset, SHARED_DIGITS)\n",
    "shared_loader = DataLoader(shared_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "H_A_shared, _ = get_hidden_activations(model_A, shared_loader)\n",
    "H_B_shared, _ = get_hidden_activations(model_B, shared_loader)\n",
    "\n",
    "# Compute rotation matrix R\n",
    "if H_A_shared.shape[0] > 0:\n",
    "    R_np, _ = orthogonal_procrustes(H_B_shared.numpy(), H_A_shared.numpy())\n",
    "    R = torch.tensor(R_np, dtype=torch.float32, device=DEVICE)\n",
    "    print(f\"Alignment matrix R computed, shape: {R.shape}\")\n",
    "else:\n",
    "    R = torch.eye(feature_dim, device=DEVICE)\n",
    "    print(\"Warning: Using identity matrix for alignment\")\n",
    "\n",
    "# Transport probe: W_tilde_4 = R @ W4\n",
    "W_tilde_4 = (R @ W4.unsqueeze(1)).squeeze(1)\n",
    "print(f\"Transported probe computed, shape: {W_tilde_4.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5-7: Surgical edit and add output weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate behavior region and perform surgery\n",
    "W_clf_A = model_A.fc3.weight.data.clone().detach()\n",
    "\n",
    "# Calculate cosine similarities\n",
    "cosine_sims = torch.nn.functional.cosine_similarity(\n",
    "    W_clf_A, W_tilde_4.unsqueeze(0), dim=1\n",
    ")\n",
    "\n",
    "# Select rows with most negative similarity\n",
    "selected_indices = torch.argsort(cosine_sims)[:K_ROWS]\n",
    "print(f\"Selected rows for surgery: {selected_indices.tolist()}\")\n",
    "print(f\"Their cosine similarities: {cosine_sims[selected_indices].tolist()}\")\n",
    "\n",
    "# Create surgically modified model\n",
    "surgically_modified_model_A = SimpleNN().to(DEVICE)\n",
    "surgically_modified_model_A.load_state_dict(model_A.state_dict())\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Apply surgical edits\n",
    "    for idx in selected_indices:\n",
    "        surgically_modified_model_A.fc3.weight.data[idx] += ALPHA * W_tilde_4\n",
    "    \n",
    "    # Copy class 4 weights from Model B\n",
    "    surgically_modified_model_A.fc3.weight.data[TARGET_DIGIT] = model_B.fc3.weight.data[TARGET_DIGIT]\n",
    "    surgically_modified_model_A.fc3.bias.data[TARGET_DIGIT] = model_B.fc3.bias.data[TARGET_DIGIT]\n",
    "\n",
    "print(\"Surgical modification complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Evaluation and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test datasets\n",
    "original_digits_test = create_subset(full_test_dataset, CLASS1_LABELS)\n",
    "target_digit_test = create_subset(full_test_dataset, [TARGET_DIGIT])\n",
    "ooc_digit_test = create_subset(full_test_dataset, [5])  # Out-of-class digit\n",
    "\n",
    "# Evaluate models\n",
    "def evaluate_surgery_results():\n",
    "    print(\"\\n=== SURGERY RESULTS ===\")\n",
    "    print(\"\\nOriginal Model A:\")\n",
    "    orig_acc_0123 = evaluate_model(model_A, DataLoader(original_digits_test, BATCH_SIZE))\n",
    "    orig_acc_4 = evaluate_model(model_A, DataLoader(target_digit_test, BATCH_SIZE))\n",
    "    orig_acc_5 = evaluate_model(model_A, DataLoader(ooc_digit_test, BATCH_SIZE))\n",
    "    print(f\"  Digits 0,1,2,3: {orig_acc_0123:.2f}%\")\n",
    "    print(f\"  Digit 4: {orig_acc_4:.2f}%\")\n",
    "    print(f\"  Digit 5: {orig_acc_5:.2f}%\")\n",
    "    \n",
    "    print(\"\\nSurgically Modified Model A:\")\n",
    "    surg_acc_0123 = evaluate_model(surgically_modified_model_A, DataLoader(original_digits_test, BATCH_SIZE))\n",
    "    surg_acc_4 = evaluate_model(surgically_modified_model_A, DataLoader(target_digit_test, BATCH_SIZE))\n",
    "    surg_acc_5 = evaluate_model(surgically_modified_model_A, DataLoader(ooc_digit_test, BATCH_SIZE))\n",
    "    print(f\"  Digits 0,1,2,3: {surg_acc_0123:.2f}%\")\n",
    "    print(f\"  Digit 4: {surg_acc_4:.2f}%\")\n",
    "    print(f\"  Digit 5: {surg_acc_5:.2f}%\")\n",
    "    \n",
    "    print(\"\\nModel B (reference):\")\n",
    "    ref_acc_2345 = evaluate_model(model_B, DataLoader(create_subset(full_test_dataset, CLASS2_LABELS), BATCH_SIZE))\n",
    "    ref_acc_4 = evaluate_model(model_B, DataLoader(target_digit_test, BATCH_SIZE))\n",
    "    print(f\"  Digits 2,3,4,5: {ref_acc_2345:.2f}%\")\n",
    "    print(f\"  Digit 4: {ref_acc_4:.2f}%\")\n",
    "    \n",
    "    return {\n",
    "        'preservation': surg_acc_0123,\n",
    "        'transfer': surg_acc_4,\n",
    "        'specificity': surg_acc_5\n",
    "    }\n",
    "\n",
    "results = evaluate_surgery_results()\n",
    "\n",
    "print(\"\\n=== SURGERY SUCCESS METRICS ===\")\n",
    "print(f\"Preservation (should be ~99%): {results['preservation']:.2f}%\")\n",
    "print(f\"Transfer (should be >random 10%): {results['transfer']:.2f}%\")\n",
    "print(f\"Specificity (should be ~0%): {results['specificity']:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}