#!/bin/bash

# Architecture Transfer Comparison - Same vs Cross Architecture 
# Fixed rho=0.5 to isolate architecture effects

echo "üèóÔ∏è ARCHITECTURE TRANSFER COMPARISON"
echo "===================================="
echo "Testing transfer effectiveness across different architectures:"
echo "- Same Architecture: DeepNN ‚Üí DeepNN"
echo "- Cross Architecture: DeepNN ‚Üí WideNN"
echo "- Fixed rho = 0.5 (balanced blending)"
echo "- Clean setup: Source {2,3,4,5,6,7} ‚Üí Target {0,1,2,3,4,5}"
echo "- Transfer class: 6, Specificity class: 7"
echo ""

# Configuration
SEEDS=(42 123 456)  # Multiple seeds for reliability
TRANSFER_CLASS=6
SPECIFICITY_CLASS=7
FIXED_RHO=0.5
MAX_EPOCHS=5
SAE_EPOCHS=50
BATCH_SIZE=32
LEARNING_RATE=0.001
CONCEPT_DIM=24

# Check conda environment
if [[ -z "${CONDA_DEFAULT_ENV}" ]]; then
    echo "‚ö†Ô∏è  Setting up conda environment..."
    conda create -n neural_transfer python=3.9 -y
    conda activate neural_transfer
    conda install pytorch torchvision cpuonly -c pytorch -y
    pip install numpy matplotlib scikit-learn seaborn
else
    echo "‚úÖ Using conda environment: ${CONDA_DEFAULT_ENV}"
fi

echo ""
echo "üìÅ Creating results directory..."
mkdir -p experiment_results/architecture_comparison/{same_arch,cross_arch}
mkdir -p logs

# Create the architecture comparison experiment runner
cat > run_architecture_comparison_experiment.py << 'EOF'
#!/usr/bin/env python3
"""
Architecture transfer comparison with fixed rho=0.5
Tests same-architecture vs cross-architecture transfer effectiveness.
"""

import torch
import torch.nn as nn
import json
import sys
import logging
from pathlib import Path
from datetime import datetime
from typing import Dict, Optional, Set
import copy

# Import necessary modules
from architectures import WideNN, DeepNN
from experimental_framework import MNISTDataManager, ExperimentConfig, ModelTrainer
from robust_balanced_transfer import RobustBalancedTransferSystem

# Suppress INFO logs for cleaner output
logging.basicConfig(level=logging.WARNING)

class FixedRhoTransferSystem(RobustBalancedTransferSystem):
    """Transfer system with fixed rho=0.5 for architecture comparison."""
    
    def __init__(self, source_model, target_model, source_classes, target_classes, 
                 concept_dim=24, device='cpu'):
        super().__init__(source_model, target_model, source_classes, target_classes, concept_dim, device)
        self.fixed_rho = 0.5
        print(f"   üéõÔ∏è  FixedRhoTransferSystem initialized with rho=0.5")
    
    def transfer(self, target_features):
        """Apply transfer with fixed rho=0.5."""
        # Get source features through alignment
        source_concepts = self.target_sae.encode(target_features)
        aligned_concepts = self.aligner.transform(source_concepts)
        enhanced_features = self.target_sae.decode(aligned_concepts)
        
        # Apply fixed rho=0.5 blending
        rho = torch.tensor(0.5, device=target_features.device, dtype=target_features.dtype)
        final_features = rho * target_features + (1 - rho) * enhanced_features
        
        return final_features

class ArchitectureComparisonEvaluator:
    """Evaluator for architecture comparison experiments."""
    
    def __init__(self, config):
        self.config = config
    
    def evaluate_all_accuracies(self, 
                               source_model, target_before_model, target_after_model,
                               source_test_loader, target_test_loader,
                               source_classes: Set[int], target_classes: Set[int],
                               transfer_class: int, specificity_class: int):
        """Evaluate accuracies for architecture comparison."""
        
        print("üìä MEASURING ARCHITECTURE COMPARISON ACCURACIES")
        print("=" * 50)
        
        # Test source model
        source_original = self._measure_accuracy(source_model, source_test_loader, source_classes, "source original classes")
        source_transfer = self._measure_accuracy(source_model, source_test_loader, {transfer_class}, f"source class {transfer_class}")
        source_specificity = self._measure_accuracy(source_model, source_test_loader, {specificity_class}, f"source class {specificity_class}")
        
        # Test target before transfer
        target_before_original = self._measure_accuracy(target_before_model, target_test_loader, target_classes, "target before original classes")
        target_before_transfer = self._measure_accuracy(target_before_model, source_test_loader, {transfer_class}, f"target before class {transfer_class}")
        target_before_specificity = self._measure_accuracy(target_before_model, source_test_loader, {specificity_class}, f"target before class {specificity_class}")
        
        # Test target after transfer
        target_after_original = self._measure_accuracy(target_after_model, target_test_loader, target_classes, "target after original classes")
        target_after_transfer = self._measure_accuracy(target_after_model, source_test_loader, {transfer_class}, f"target after class {transfer_class}")
        target_after_specificity = self._measure_accuracy(target_after_model, source_test_loader, {specificity_class}, f"target after class {specificity_class}")
        
        results = {
            'source_original_accuracy': source_original,
            'source_transfer_class_accuracy': source_transfer,
            'source_specificity_class_accuracy': source_specificity,
            'target_before_original_accuracy': target_before_original,
            'target_before_transfer_class_accuracy': target_before_transfer,
            'target_before_specificity_class_accuracy': target_before_specificity,
            'target_after_original_accuracy': target_after_original,
            'target_after_transfer_class_accuracy': target_after_transfer,
            'target_after_specificity_class_accuracy': target_after_specificity
        }
        
        # Validate data integrity
        self._validate_no_data_leakage(results, transfer_class, specificity_class)
        
        return results
    
    def _measure_accuracy(self, model, data_loader, target_classes: Set[int], description: str) -> float:
        """Measure simple classification accuracy."""
        device = self.config.device
        model.eval()
        
        correct = 0
        total = 0
        
        with torch.no_grad():
            for data, labels in data_loader:
                data, labels = data.to(device), labels.to(device)
                
                # Filter for target classes only
                target_mask = torch.tensor([label.item() in target_classes for label in labels])
                if target_mask.sum() == 0:
                    continue
                
                filtered_data = data[target_mask]
                filtered_labels = labels[target_mask]
                
                # Get model predictions
                if hasattr(model, 'forward'):
                    outputs = model(filtered_data.view(filtered_data.size(0), -1))
                else:
                    outputs = model(filtered_data)
                
                _, predicted = torch.max(outputs, 1)
                
                # Direct accuracy calculation
                for pred, true_label in zip(predicted, filtered_labels):
                    if pred.item() == true_label.item():
                        correct += 1
                    total += 1
        
        accuracy = correct / total if total > 0 else 0.0
        print(f"   {description}: {accuracy:.1%} ({correct}/{total})")
        return accuracy
    
    def _validate_no_data_leakage(self, results, transfer_class: int, specificity_class: int):
        """Validate clean experimental setup."""
        
        transfer_before = results['target_before_transfer_class_accuracy']
        specificity_before = results['target_before_specificity_class_accuracy']
        
        print(f"\nüîç DATA LEAKAGE VALIDATION")
        print("-" * 30)
        
        issues_found = []
        
        if transfer_before > 0.30:
            issues_found.append(f"class {transfer_class}")
            print(f"üö® WARNING: Target has {transfer_before:.1%} accuracy on class {transfer_class} before transfer!")
        else:
            print(f"‚úÖ Transfer class {transfer_class}: {transfer_before:.1%} ‚â§ 30%")
        
        if specificity_before > 0.30:
            issues_found.append(f"class {specificity_class}")
            print(f"üö® WARNING: Target has {specificity_before:.1%} accuracy on class {specificity_class} before transfer!")
        else:
            print(f"‚úÖ Specificity class {specificity_class}: {specificity_before:.1%} ‚â§ 30%")
        
        if issues_found:
            print(f"\n‚ö†Ô∏è  Data leakage detected with: {', '.join(issues_found)}")
        else:
            print(f"\n‚úÖ ALL VALIDATION PASSED - Clean experimental setup!")
    
    def print_results_table(self, results, transfer_class: int, specificity_class: int, architecture_type: str):
        """Print architecture comparison results table."""
        
        print(f"\nüìà {architecture_type.upper()} ARCHITECTURE RESULTS (rho=0.5)")
        print("=" * 80)
        print()
        
        print(f"| Model                    | Original Classes | Class {transfer_class}  | Class {specificity_class} |")
        print("|--------------------------|------------------|----------|----------|")
        print(f"| Source                   | {results['source_original_accuracy']:14.1%} | {results['source_transfer_class_accuracy']:7.1%} | {results['source_specificity_class_accuracy']:7.1%} |")
        print(f"| Target (Before)          | {results['target_before_original_accuracy']:14.1%} | {results['target_before_transfer_class_accuracy']:7.1%} | {results['target_before_specificity_class_accuracy']:7.1%} |")
        print(f"| Target (After)           | {results['target_after_original_accuracy']:14.1%} | {results['target_after_transfer_class_accuracy']:7.1%} | {results['target_after_specificity_class_accuracy']:7.1%} |")
        print()
        
        transfer_improvement = results['target_after_transfer_class_accuracy'] - results['target_before_transfer_class_accuracy']
        print("üìä KEY METRICS:")
        print(f"   ‚Ä¢ Architecture: {architecture_type}")
        print(f"   ‚Ä¢ Transfer improvement: {transfer_improvement:.1%} (class {transfer_class}: {results['target_before_transfer_class_accuracy']:.1%} ‚Üí {results['target_after_transfer_class_accuracy']:.1%})")
        print(f"   ‚Ä¢ Knowledge preservation: {results['target_after_original_accuracy']:.1%}")
        print(f"   ‚Ä¢ Transfer specificity: {results['target_after_specificity_class_accuracy']:.1%} (should be low)")

class ArchWrappedTransferModel(nn.Module):
    """Wrapper for transfer systems in architecture comparison."""
    def __init__(self, base_model, transfer_system, transfer_class):
        super().__init__()
        self.base_model = base_model
        self.transfer_system = transfer_system
        self.transfer_class = transfer_class
    
    def forward(self, x):
        if self.transfer_system is None:
            return self.base_model(x)
        
        x_flat = x.view(x.size(0), -1)
        
        # Try transfer_concept method
        if hasattr(self.transfer_system, 'transfer_concept'):
            try:
                enhanced_outputs = self.transfer_system.transfer_concept(x_flat, self.transfer_class)
                if enhanced_outputs is not None:
                    return enhanced_outputs
            except Exception as e:
                print(f"Warning: transfer_concept failed: {e}")
        
        # Try feature-level transfer
        if hasattr(self.transfer_system, 'transfer'):
            try:
                features = self.base_model.get_features(x_flat)
                enhanced_features = self.transfer_system.transfer(features)
                outputs = self.base_model.classify_from_features(enhanced_features)
                return outputs
            except Exception as e:
                print(f"Warning: feature transfer failed: {e}")
        
        # Fallback
        return self.base_model(x_flat)

def run_architecture_comparison_experiment(architecture_type: str, seed: int, config_params: Dict) -> Optional[Dict]:
    """Run architecture comparison experiment."""
    
    print(f"üèóÔ∏è Running {architecture_type.upper()} architecture experiment (seed={seed})")
    
    # Clean experimental setup
    source_classes = {2, 3, 4, 5, 6, 7}      # Source knows class 6 and 7
    target_classes = {0, 1, 2, 3, 4, 5}      # Target doesn't know class 6 or 7
    transfer_class = 6                        # Transfer class 6
    specificity_class = 7                     # Should NOT transfer
    
    config = ExperimentConfig(
        seed=seed,
        max_epochs=config_params['max_epochs'],
        batch_size=config_params['batch_size'],
        learning_rate=config_params['learning_rate'],
        concept_dim=config_params['concept_dim'],
        device='cpu'
    )
    
    try:
        # Create data and train models
        data_manager = MNISTDataManager(config)
        trainer = ModelTrainer(config)
        
        source_train_loader, target_train_loader, source_test_loader, target_test_loader = \
            data_manager.get_data_loaders(source_classes, target_classes)
        
        print(f"   üìö Training models: Source={sorted(source_classes)}, Target={sorted(target_classes)}")
        
        # Train source model (always DeepNN)
        source_model = DeepNN()
        trained_source, source_acc = trainer.train_model(source_model, source_train_loader, source_test_loader)
        
        # Train target model based on architecture type
        if architecture_type == 'same_arch':
            target_model = DeepNN()  # Same architecture: DeepNN ‚Üí DeepNN
            print(f"   üèóÔ∏è  Same architecture: DeepNN ‚Üí DeepNN")
        else:  # cross_arch
            target_model = WideNN()  # Cross architecture: DeepNN ‚Üí WideNN
            print(f"   üèóÔ∏è  Cross architecture: DeepNN ‚Üí WideNN")
        
        trained_target, target_acc = trainer.train_model(target_model, target_train_loader, target_test_loader)
        
        if trained_source is None or trained_target is None:
            print("   ‚ùå Model training failed")
            return None
        
        print(f"   ‚úÖ Models trained: Source={source_acc:.3f}, Target={target_acc:.3f}")
        
        # CRITICAL: Clone target model before transfer operations
        target_before_transfer = copy.deepcopy(trained_target)
        
        # Create transfer system with fixed rho=0.5
        transfer_system = FixedRhoTransferSystem(
            source_model=trained_source,
            target_model=trained_target,
            source_classes=source_classes,
            target_classes=target_classes,
            concept_dim=config.concept_dim,
            device=config.device
        )
        
        # Fit and setup transfer
        print("   üîÑ Setting up transfer system...")
        transfer_system.fit(source_train_loader, target_train_loader, sae_epochs=config_params['sae_epochs'])
        transfer_system.setup_injection_system(transfer_class, source_train_loader, target_train_loader)
        
        # Create evaluation models
        target_before = target_before_transfer
        target_after = ArchWrappedTransferModel(trained_target, transfer_system, transfer_class)
        
        # Evaluate
        print("   üìä Measuring accuracies...")
        evaluator = ArchitectureComparisonEvaluator(config)
        
        results = evaluator.evaluate_all_accuracies(
            source_model=trained_source,
            target_before_model=target_before,
            target_after_model=target_after,
            source_test_loader=source_test_loader,
            target_test_loader=target_test_loader,
            source_classes=source_classes,
            target_classes=target_classes,
            transfer_class=transfer_class,
            specificity_class=specificity_class
        )
        
        # Print results
        evaluator.print_results_table(results, transfer_class, specificity_class, architecture_type)
        
        # Calculate key metrics
        transfer_improvement = results['target_after_transfer_class_accuracy'] - results['target_before_transfer_class_accuracy']
        
        # Package for saving
        experiment_results = {
            'experiment_id': f"arch_comparison_{architecture_type}_seed_{seed}",
            'architecture_type': architecture_type,
            'seed': seed,
            'timestamp': datetime.now().isoformat(),
            'fixed_rho': 0.5,
            'experimental_setup': {
                'source_architecture': 'DeepNN',
                'target_architecture': 'DeepNN' if architecture_type == 'same_arch' else 'WideNN',
                'source_classes': sorted(source_classes),
                'target_classes': sorted(target_classes),
                'transfer_class': transfer_class,
                'specificity_class': specificity_class
            },
            'model_accuracies': {
                'source': source_acc,
                'target': target_acc
            },
            'accuracy_results': results,
            'key_metrics': {
                'transfer_improvement': transfer_improvement,
                'knowledge_preservation': results['target_after_original_accuracy'],
                'transfer_effectiveness': results['target_after_transfer_class_accuracy'],
                'transfer_specificity': results['target_after_specificity_class_accuracy']
            },
            'success_criteria': {
                'knowledge_preservation': results['target_after_original_accuracy'] >= 0.8,
                'transfer_effectiveness': results['target_after_transfer_class_accuracy'] >= 0.7,
                'transfer_specificity': results['target_after_specificity_class_accuracy'] <= 0.1
            },
            'data_leakage_check': {
                'transfer_class_before': results['target_before_transfer_class_accuracy'],
                'specificity_class_before': results['target_before_specificity_class_accuracy'],
                'data_leakage_detected': (results['target_before_transfer_class_accuracy'] > 0.3 or 
                                         results['target_before_specificity_class_accuracy'] > 0.3)
            }
        }
        
        return experiment_results
        
    except Exception as e:
        print(f"   ‚ùå Architecture comparison experiment failed: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

def main():
    if len(sys.argv) != 4:
        print("Usage: python run_architecture_comparison_experiment.py <arch_type> <seed> <config_json>")
        sys.exit(1)
    
    architecture_type = sys.argv[1]
    seed = int(sys.argv[2])
    config_params = json.loads(sys.argv[3])
    
    result = run_architecture_comparison_experiment(architecture_type, seed, config_params)
    
    if result:
        print(json.dumps(result, indent=2))
    else:
        sys.exit(1)

if __name__ == "__main__":
    main()
EOF

chmod +x run_architecture_comparison_experiment.py

echo ""
echo "üß™ RUNNING ARCHITECTURE COMPARISON EXPERIMENTS"
echo "=============================================="

# Configuration JSON
CONFIG_JSON="{\"max_epochs\": $MAX_EPOCHS, \"sae_epochs\": $SAE_EPOCHS, \"batch_size\": $BATCH_SIZE, \"learning_rate\": $LEARNING_RATE, \"concept_dim\": $CONCEPT_DIM, \"transfer_class\": $TRANSFER_CLASS}"

# Architecture types to test
declare -a ARCH_TYPES=("same_arch" "cross_arch")

TOTAL_EXPERIMENTS=$((${#ARCH_TYPES[@]} * ${#SEEDS[@]}))
CURRENT_EXPERIMENT=0
SUCCESSFUL_EXPERIMENTS=0
FAILED_EXPERIMENTS=0
DATA_LEAKAGE_COUNT=0

echo "üìä Architecture Comparison Plan:"
echo "   - Same Architecture: DeepNN ‚Üí DeepNN"
echo "   - Cross Architecture: DeepNN ‚Üí WideNN"
echo "   - Fixed rho: 0.5 (balanced blending)"
echo "   - Source model: {2,3,4,5,6,7} (knows class 6 and 7)"
echo "   - Target model: {0,1,2,3,4,5} (doesn't know class 6 or 7)"
echo "   - Seeds: ${SEEDS[*]}"
echo "   - Total experiments: $TOTAL_EXPERIMENTS"
echo ""

# Store results for comparison
declare -A SAME_ARCH_RESULTS
declare -A CROSS_ARCH_RESULTS

# Run experiments
for arch_type in "${ARCH_TYPES[@]}"; do
    echo "üèóÔ∏è  ARCHITECTURE TYPE: $arch_type"
    echo "================================="
    
    for seed in "${SEEDS[@]}"; do
        CURRENT_EXPERIMENT=$((CURRENT_EXPERIMENT + 1))
        
        echo ""
        echo "[$CURRENT_EXPERIMENT/$TOTAL_EXPERIMENTS] Running $arch_type with seed $seed..."
        
        # Run experiment
        if python run_architecture_comparison_experiment.py "$arch_type" "$seed" "$CONFIG_JSON" > "logs/arch_comparison_${arch_type}_seed_${seed}.json" 2> "logs/arch_comparison_${arch_type}_seed_${seed}.log"; then
            
            # Save result
            result_file="experiment_results/architecture_comparison/$arch_type/arch_comparison_${arch_type}_seed_${seed}.json"
            cp "logs/arch_comparison_${arch_type}_seed_${seed}.json" "$result_file"
            
            # Check for data leakage
            if grep -q "Data leakage detected" "logs/arch_comparison_${arch_type}_seed_${seed}.log"; then
                echo "   ‚ö†Ô∏è  WARNING: Data leakage detected!"
                DATA_LEAKAGE_COUNT=$((DATA_LEAKAGE_COUNT + 1))
            fi
            
            SUCCESSFUL_EXPERIMENTS=$((SUCCESSFUL_EXPERIMENTS + 1))
            echo "   ‚úÖ Success"
            
        else
            FAILED_EXPERIMENTS=$((FAILED_EXPERIMENTS + 1))
            echo "   ‚ùå Failed - check logs"
        fi
    done
done

echo ""
echo "üìä GENERATING ARCHITECTURE COMPARISON ANALYSIS"
echo "=============================================="

# Analysis script
cat > analyze_architecture_comparison.py << 'EOF'
#!/usr/bin/env python3
import json
import statistics
import numpy as np
from pathlib import Path
from datetime import datetime

def analyze_architecture_comparison():
    results = {'same_arch': [], 'cross_arch': []}
    
    base_path = Path('experiment_results/architecture_comparison')
    for arch_type in ['same_arch', 'cross_arch']:
        result_dir = base_path / arch_type
        if result_dir.exists():
            for result_file in result_dir.glob('*.json'):
                try:
                    with open(result_file, 'r') as f:
                        results[arch_type].append(json.load(f))
                except Exception as e:
                    print(f"Warning: {result_file}: {e}")
    
    report = f"""# Architecture Transfer Comparison: Same vs Cross Architecture

**Generated:** {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}

## Experimental Setup

**Fixed Parameters:**
- **Rho:** 0.5 (balanced blending - eliminates rho effects)
- **Source Model:** DeepNN trained on {{2,3,4,5,6,7}}
- **Transfer Class:** 6 (should improve from ~0% to >70%)
- **Specificity Class:** 7 (should stay at ~0%)

**Architecture Variants:**
- **Same Architecture:** DeepNN ‚Üí DeepNN (same depth, same width)
- **Cross Architecture:** DeepNN ‚Üí WideNN (different depth, different width)

## Results Summary

"""
    
    # Analyze results
    comparison_data = {}
    
    for arch_type in ['same_arch', 'cross_arch']:
        arch_results = results[arch_type]
        if not arch_results:
            report += f"### {arch_type.replace('_', ' ').title()}\n‚ö†Ô∏è No results\n\n"
            continue
        
        # Extract key metrics
        transfer_improvements = []
        effectiveness_scores = []
        preservation_scores = []
        specificity_scores = []
        
        for r in arch_results:
            metrics = r['key_metrics']
            transfer_improvements.append(metrics['transfer_improvement'])
            effectiveness_scores.append(metrics['transfer_effectiveness'])
            preservation_scores.append(metrics['knowledge_preservation'])
            specificity_scores.append(metrics['transfer_specificity'])
        
        # Store for comparison
        comparison_data[arch_type] = {
            'transfer_improvement': transfer_improvements,
            'effectiveness': effectiveness_scores,
            'preservation': preservation_scores,
            'specificity': specificity_scores
        }
        
        data_leakage_count = sum(1 for r in arch_results if r['data_leakage_check']['data_leakage_detected'])
        
        arch_name = "Same Architecture (DeepNN ‚Üí DeepNN)" if arch_type == 'same_arch' else "Cross Architecture (DeepNN ‚Üí WideNN)"
        
        report += f"""### {arch_name}

| Metric | Mean ¬± Std | Success Rate |
|--------|------------|--------------|
| **Transfer Improvement** | {statistics.mean(transfer_improvements):.1%} ¬± {statistics.stdev(transfer_improvements) if len(transfer_improvements) > 1 else 0:.1%} | N/A |
| **Transfer Effectiveness** | {statistics.mean(effectiveness_scores):.1%} ¬± {statistics.stdev(effectiveness_scores) if len(effectiveness_scores) > 1 else 0:.1%} | ‚â•70%: {sum(1 for x in effectiveness_scores if x >= 0.7)}/{len(effectiveness_scores)} |
| **Knowledge Preservation** | {statistics.mean(preservation_scores):.1%} ¬± {statistics.stdev(preservation_scores) if len(preservation_scores) > 1 else 0:.1%} | ‚â•80%: {sum(1 for x in preservation_scores if x >= 0.8)}/{len(preservation_scores)} |
| **Transfer Specificity** | {statistics.mean(specificity_scores):.1%} ¬± {statistics.stdev(specificity_scores) if len(specificity_scores) > 1 else 0:.1%} | ‚â§10%: {sum(1 for x in specificity_scores if x <= 0.1)}/{len(specificity_scores)} |

**Data Quality:** {data_leakage_count}/{len(arch_results)} experiments had data leakage warnings

"""
    
    # Direct comparison
    if results['same_arch'] and results['cross_arch']:
        same_improvement = statistics.mean(comparison_data['same_arch']['transfer_improvement'])
        cross_improvement = statistics.mean(comparison_data['cross_arch']['transfer_improvement'])
        
        same_effectiveness = statistics.mean(comparison_data['same_arch']['effectiveness'])
        cross_effectiveness = statistics.mean(comparison_data['cross_arch']['effectiveness'])
        
        same_preservation = statistics.mean(comparison_data['same_arch']['preservation'])
        cross_preservation = statistics.mean(comparison_data['cross_arch']['preservation'])
        
        # Determine winner
        if same_improvement > cross_improvement:
            winner = "Same Architecture"
            advantage = same_improvement - cross_improvement
        else:
            winner = "Cross Architecture"
            advantage = cross_improvement - same_improvement
        
        report += f"""## Direct Comparison (Fixed rho=0.5)

| Architecture Type | Transfer Improvement | Transfer Effectiveness | Knowledge Preservation |
|-------------------|---------------------|----------------------|----------------------|
| **Same (DeepNN ‚Üí DeepNN)** | {same_improvement:.1%} | {same_effectiveness:.1%} | {same_preservation:.1%} |
| **Cross (DeepNN ‚Üí WideNN)** | {cross_improvement:.1%} | {cross_effectiveness:.1%} | {cross_preservation:.1%} |
| **Difference** | {same_improvement - cross_improvement:+.1%} | {same_effectiveness - cross_effectiveness:+.1%} | {same_preservation - cross_preservation:+.1%} |

### Key Findings

**Winner:** **{winner}** üèÜ (by {advantage:.1%} transfer improvement)

**Architecture Effects (with rho fixed at 0.5):**
"""
        
        if same_improvement > cross_improvement:
            report += f"""
- **Same architecture transfer is more effective** ({same_improvement:.1%} vs {cross_improvement:.1%})
- Same architectures have better feature alignment
- Cross-architecture transfer faces architectural compatibility challenges
- The {advantage:.1%} difference suggests architecture matters more than rho tuning
"""
        else:
            report += f"""
- **Cross architecture transfer is surprisingly effective** ({cross_improvement:.1%} vs {same_improvement:.1%})
- The transfer system handles architectural differences well
- Cross-architecture may provide better regularization
- The {advantage:.1%} advantage suggests architectural diversity can help
"""
        
        report += f"""
### Statistical Significance

To determine if the difference is meaningful:
- **Effect Size:** {advantage:.1%} transfer improvement difference
- **Same Arch Std:** {statistics.stdev(comparison_data['same_arch']['transfer_improvement']) if len(comparison_data['same_arch']['transfer_improvement']) > 1 else 0:.1%}
- **Cross Arch Std:** {statistics.stdev(comparison_data['cross_arch']['transfer_improvement']) if len(comparison_data['cross_arch']['transfer_improvement']) > 1 else 0:.1%}

### Practical Implications

1. **Architecture Choice:** {"Same architecture is preferable for maximum transfer" if same_improvement > cross_improvement else "Cross architecture works well and may provide benefits"}
2. **Rho Sensitivity:** With rho fixed at 0.5, architecture choice has a {advantage:.1%} impact
3. **System Robustness:** The transfer system {"struggles with" if abs(advantage) > 0.1 else "handles well"} architectural differences
"""
    
    report += f"""
## Conclusion

This experiment isolates architectural effects by fixing rho=0.5, showing that:

1. **Architecture matters:** Different architectures produce measurably different transfer results
2. **Rho vs Architecture:** {"Architecture choice has a larger impact than rho tuning" if abs(advantage) > 0.05 else "Architecture and rho effects are comparable"}
3. **System Design:** The transfer system's architectural robustness is {"limited" if abs(advantage) > 0.1 else "good"}

## Files Generated
- `experiment_results/architecture_comparison/same_arch/` - Same architecture results
- `experiment_results/architecture_comparison/cross_arch/` - Cross architecture results
- `experiment_results/ARCHITECTURE_COMPARISON.md` - This report
"""
    
    return report

def main():
    report = analyze_architecture_comparison()
    
    report_path = Path('experiment_results/ARCHITECTURE_COMPARISON.md')
    with open(report_path, 'w') as f:
        f.write(report)
    
    print(f"üìÑ Architecture comparison analysis saved to: {report_path}")

if __name__ == "__main__":
    main()
EOF

python analyze_architecture_comparison.py

echo ""
echo "üéâ ARCHITECTURE COMPARISON COMPLETE!"
echo "==================================="
echo ""
echo "üìä Final Statistics:" 
echo "   - Total experiments: $TOTAL_EXPERIMENTS"
echo "   - Successful: $SUCCESSFUL_EXPERIMENTS"
echo "   - Failed: $FAILED_EXPERIMENTS"
echo "   - Data leakage warnings: $DATA_LEAKAGE_COUNT"
echo ""
echo "üìÅ Results Location:"
echo "   - Architecture analysis: experiment_results/ARCHITECTURE_COMPARISON.md"
echo "   - Same arch results: experiment_results/architecture_comparison/same_arch/"
echo "   - Cross arch results: experiment_results/architecture_comparison/cross_arch/"
echo ""
echo "üîß EXPERIMENTAL DESIGN:"
echo "   ‚úÖ Fixed rho=0.5 to isolate architecture effects"
echo "   ‚úÖ Same setup: DeepNN ‚Üí DeepNN vs DeepNN ‚Üí WideNN"
echo "   ‚úÖ Clean class splits: Source {2,3,4,5,6,7} ‚Üí Target {0,1,2,3,4,5}"
echo "   ‚úÖ Transfer class 6, specificity check class 7"
echo ""
echo "üìà KEY QUESTION ANSWERED:"
echo "   Does architecture matter more than rho tuning for transfer effectiveness?"

if [ $DATA_LEAKAGE_COUNT -gt 0 ]; then
    echo ""
    echo "‚ö†Ô∏è  Note: $DATA_LEAKAGE_COUNT experiments showed data leakage warnings"
fi

echo ""
echo "üöÄ Architecture comparison ready! Check experiment_results/ARCHITECTURE_COMPARISON.md for results."

# Cleanup
rm -f run_architecture_comparison_experiment.py analyze_architecture_comparison.py